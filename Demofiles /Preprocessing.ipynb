{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Char to Word(Squad) - Done\n",
    "# Conversion the whole data into reading data with relevant information - Done\n",
    "# Tokenizing the context for data extraction - Done\n",
    "# Preprocessing the data- Word2vec and Char Embedding\n",
    "# Making neccesary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/squad/train-v1.1.json') as json_data:\n",
    "    train_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_context(context):\n",
    "    tokens = []\n",
    "    for token in context:\n",
    "        flag = False\n",
    "        l = (\"-\", \"\\u2212\", \"\\u2014\", \"\\u2013\", \"/\", \"~\", '\"', \"'\", \"\\u201C\", \"\\u2019\", \"\\u201D\", \"\\u2018\", \"\\u00B0\")\n",
    "        tokens.extend(re.split(\"([{}])\".format(\"\".join(l)), token))\n",
    "    return tokens\n",
    "\n",
    "def word_tokenize(tokens):\n",
    "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]\n",
    "\n",
    "sent_tokenize = nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_spans(context, each_word):\n",
    "    span = []\n",
    "    start_index = 0\n",
    "    for tokens in each_word:\n",
    "        spans = []\n",
    "        for token in tokens:\n",
    "            start_index = context.find(token, start_index) # getting start index of each word after a particular index\n",
    "            spans.append((start_index, start_index + len(token))) # appending that start and end index into list\n",
    "            start_index += len(token) # updating the start index to check ahead\n",
    "        span.append(spans)\n",
    "    return span\n",
    "\n",
    "def get_word_span(context, each_word, start, stop):\n",
    "    each_word_span = get_2d_spans(context, each_word)\n",
    "    word_span_list = []\n",
    "    for sent_index, word_span in enumerate(each_word_span):\n",
    "        for word_index, char_span in enumerate(word_span):\n",
    "             if (stop >= char_span[0] and start <= char_span[1]):\n",
    "                word_span_list.append((sent_index, word_index))\n",
    "    return word_span_list[0], (word_span_list[-1][0], word_span_list[-1][1] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data = []\n",
    "    data_points = {}\n",
    "    each_context = []\n",
    "    main_data = []\n",
    "    for data_index, data_paragraph in enumerate(train_data['data']):\n",
    "        for each_paragraph in data_paragraph['paragraphs']:\n",
    "            question_data = each_paragraph['qas']\n",
    "            context_data = each_paragraph['context']\n",
    "            for number_of_question in range(len(question_data)):\n",
    "                    data_points['context'] = context_data\n",
    "                    data_points['question'] = question_data[number_of_question]['question']\n",
    "                    answer = question_data[number_of_question]['answers'][0]['text']\n",
    "                    data_points['answers'] = answer\n",
    "                    start = question_data[number_of_question]['answers'][0]['answer_start']\n",
    "                    stop = start + len(answer) \n",
    "                    tokenized_context = list(map(word_tokenize, sent_tokenize(context_data)))\n",
    "                    tokenized_context = [process_context(tokens) for tokens in tokenized_context]\n",
    "                    y0, y1 = get_word_span(data_points['context'], tokenized_context, start, stop)\n",
    "                    data_points['answer_start'] = y0\n",
    "                    data_points['answer_stop'] = y1\n",
    "                    data.append(data_points)\n",
    "                    data_points = {}\n",
    "            each_context.append(data)\n",
    "            data = []\n",
    "        main_data.append(each_context)\n",
    "        each_context = []\n",
    "    return main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
